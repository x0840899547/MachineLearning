# -*- coding: utf-8 -*-
"""Copy of Css324f21_hw_ml_Pram&KT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iOMCXdoqRTPD9C8sTehuDVa1BN83y4Kp

# CSS324 Homework Assignment

CIFAR10 is a small image classification dataset. Its objective is to classification an 32x32 color image into 10 classes.

See https://www.cs.toronto.edu/~kriz/cifar.html and https://keras.io/api/datasets/cifar10/ for more details.
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Load CIFAR10 dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Plot a training example
x = x_train[11, :, :, :]
y = y_train[11][0]

plt.imshow(x)
print(y)        # 7 = horse

x.shape

y_train[1]

"""## Question 1

Construct a deep neural network containing three hidden layer to classify images in the CIFAR10 dataset. You can choose the numbers of hidden nodes in three layers, appropriate activation functions, regularizers. Use 20% of the training set to validate the model.

After the training process, print the training, validation, and test accuracies, as well as plot the training loss and validation loss.
"""

# Preprocess

# Normalize input vectors
# [0, 255] --> [0.0, 1.0]
x_train = x_train / 255.0
x_test = x_test / 255.0

# # Encode target outputs using one-hot encoding

# y_train = tf.one_hot(y_train, 10)
# y_test = tf.one_hot(y_test, 10)

print(x_train.shape,y_train.shape)

# Define a model
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(32, 32,3)),
   
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(128, activation='relu',input_shape=(30,),
                          kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
    
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),			
    tf.keras.layers.Dense(64, activation='relu',input_shape=(30,),
                          kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),		
    tf.keras.layers.Dense(10, activation='softmax',input_shape=(30,))
    
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Display the model
model.summary()

# Train the model
history = model.fit(x_train, y_train, epochs=1200, batch_size=1024, shuffle=True, validation_split=0.2)

# Plot the training loss and validation loss
print(history.history)

# Evaluate the model
print(model.evaluate(x_train, y_train, verbose=0))
print(model.evaluate(x_test, y_test, verbose=0))

loss_train = np.array(history.history['loss'])
loss_test = np.array(history.history['val_loss'])

x = np.arange(0, loss_train.shape[0])
plt.plot(x, loss_train, label="Training loss")
plt.plot(x, loss_test, label="Validation loss")
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['Training loss', 'Validation loss'])
plt.show()

"""## Question 2

Construct a convolutional neural network using your own structure. Try to maximize the prediction accuracy of your model.

After the training process, print the training, validation, and test accuracies, as well as plot the training loss and validation loss.
"""

# Your implementation for Question 2
#Preprocesses
x_train= x_train/255
x_test= x_test/255

#Reshape the model
x_train = x_train.reshape((-3, 32, 32, 3))
x_test = x_test.reshape((-3, 32, 32, 3))
tf.random.set_seed(11)

#Define the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(32, 32, 3)),
    tf.keras.layers.Conv2D(64, (3,3)), 
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.MaxPooling2D((3,3), strides=(2,2)),
    tf.keras.layers.Conv2D(128, (3,3)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.MaxPooling2D((3,3), strides=(2,2)),
    tf.keras.layers.SpatialDropout2D(0.5),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu',
                          kernel_regularizer=tf.keras.regularizers.l2(1e-4)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),			
    tf.keras.layers.Dense(10, activation='softmax')
  
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Display the model
model.summary()

#Train the Model
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
 filepath='/tmp/checkpoint',
 save_weights_only=True,
 monitor = 'val_accuracy',
 mode='max',
 save_best_only=True)

history = model.fit(x_train, y_train, epochs = 1200,
                     batch_size=512,shuffle=True,
                     validation_split=0.2,callbacks=[model_checkpoint_callback])
print(history.history)

# Load the best weights
model.load_weights('/tmp/checkpoint')

print(model.evaluate(x_train, y_train, verbose=0))
print(model.evaluate(x_test, y_test, verbose=0))

# Plot the training loss and validation loss
loss_train = np.array(history.history['loss'])
loss_test = np.array(history.history['val_loss'])

x = np.arange(0, loss_train.shape[0])
plt.plot(x, loss_train, label="Training loss")
plt.plot(x, loss_test, label="Validation loss")
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['Training loss', 'Validation loss'])
plt.show()